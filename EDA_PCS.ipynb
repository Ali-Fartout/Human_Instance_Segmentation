{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "185K7kmFBXhTJh5nzT-JzZOb6iev5m2lF",
      "authorship_tag": "ABX9TyPyZjMlenBr5DR3jk+wO88X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-Fartout/People-Clothing-Segmentation/blob/main/EDA_PCS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data"
      ],
      "metadata": {
        "id": "lMWyTnE7lddM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nxVGbFBjzFn",
        "outputId": "349dc821-c3f4-4a0b-aa4d-67143a6b1fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading people-clothing-segmentation.zip to /content\n",
            " 98% 602M/616M [00:05<00:00, 111MB/s]\n",
            "100% 616M/616M [00:05<00:00, 121MB/s]\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d rajkumarl/people-clothing-segmentation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/people-clothing-segmentation.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n"
      ],
      "metadata": {
        "id": "6HBFbGD_keZz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!mkdir data/train data/test\n",
        "!mkdir data/train/image data/train/mask data/test/image data/test/mask"
      ],
      "metadata": {
        "id": "VMKO56PEotrp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd png_images/IMAGES\n",
        "!find . -type f -name \"img_0[0-7][0-9][0-9]\\.png\" -exec cp {} /content/data/train/image \\;\n",
        "!find . -type f -name \"img_0[8-9][0-9][0-9]\\.png\"  -exec cp {} /content/data/test/image \\;\n",
        "!find . -type f -name \"img_1000.png\"  -exec cp {} /content/data/test/image \\;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6ROro_zuCwM",
        "outputId": "095ea6fc-af81-44b6-a553-7f15bfff2ea6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/png_images/IMAGES\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/png_masks/MASKS\n",
        "!find . -type f -name \"seg_0[0-7][0-9][0-9]\\.png\" -exec cp {} /content/data/train/mask \\;\n",
        "!find . -type f -name \"seg_0[8-9][0-9][0-9]\\.png\"  -exec cp {} /content/data/test/mask \\;\n",
        "!find . -type f -name \"seg_1000.png\"  -exec cp {} /content/data/test/mask \\;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRc_S-wtxYvh",
        "outputId": "81ca62d1-47f0-4412-c6f1-1e77ef1aae8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/png_masks/MASKS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning data"
      ],
      "metadata": {
        "id": "BCKVGqg6lhiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_dir = os.path.join(self.root_dir, 'image')\n",
        "        self.mask_dir = os.path.join(self.root_dir, 'mask')\n",
        "        self.image_filenames = os.listdir(self.image_dir)\n",
        "        self.mask_filenames = os.listdir(self.mask_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load image and mask\n",
        "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        if self.transform is not None:\n",
        "          # apply transformations, if any\n",
        "          image, mask = self.transform(image, mask)\n",
        "        \n",
        "        # convert PIL Image to tensor\n",
        "        image = torch.tensor(np.array(image).transpose(2, 0, 1)).float()\n",
        "        mask = torch.tensor(np.array(mask)).long()\n",
        "\n",
        "        return image, mask\n"
      ],
      "metadata": {
        "id": "EATC1H2cwHe7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = SegmentationDataset('/content/data/train')\n",
        "val_dataset = SegmentationDataset('/content/data/test')\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "AoZl-MyLNjUM"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "    \n",
        "    \n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DownBlock, self).__init__()\n",
        "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
        "        self.down_sample = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_out = self.double_conv(x)\n",
        "        down_out = self.down_sample(skip_out)\n",
        "        return (down_out, skip_out)\n",
        "\n",
        "    \n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, up_sample_mode):\n",
        "        super(UpBlock, self).__init__()\n",
        "        if up_sample_mode == 'conv_transpose':\n",
        "            self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n",
        "        elif up_sample_mode == 'bilinear':\n",
        "            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n",
        "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, down_input, skip_input):\n",
        "        x = self.up_sample(down_input)\n",
        "        x = torch.cat([x, skip_input], dim=1)\n",
        "        return self.double_conv(x)\n",
        "\n",
        "    \n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, out_classes=2, up_sample_mode='conv_transpose'):\n",
        "        super(UNet, self).__init__()\n",
        "        self.up_sample_mode = up_sample_mode\n",
        "        # Downsampling Path\n",
        "        self.down_conv1 = DownBlock(3, 64)\n",
        "        self.down_conv2 = DownBlock(64, 128)\n",
        "        self.down_conv3 = DownBlock(128, 256)\n",
        "        self.down_conv4 = DownBlock(256, 512)\n",
        "        # Bottleneck\n",
        "        self.double_conv = DoubleConv(512, 1024)\n",
        "        # Upsampling Path\n",
        "        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode)\n",
        "        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode)\n",
        "        self.up_conv2 = UpBlock(128 + 256, 128, self.up_sample_mode)\n",
        "        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n",
        "        # Final Convolution\n",
        "        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, skip1_out = self.down_conv1(x)\n",
        "        x, skip2_out = self.down_conv2(x)\n",
        "        x, skip3_out = self.down_conv3(x)\n",
        "        x, skip4_out = self.down_conv4(x)\n",
        "        x = self.double_conv(x)\n",
        "        x = self.up_conv4(x, skip4_out)\n",
        "        x = self.up_conv3(x, skip3_out)\n",
        "        x = self.up_conv2(x, skip2_out)\n",
        "        x = self.up_conv1(x, skip1_out)\n",
        "        x = self.conv_last(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e8GyFEkdnbQi"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get UNet model\n",
        "model = UNet()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "bLhzGhE2DUwj"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "lLGX2jF_Moh1"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8NLdlCmHM4uR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}