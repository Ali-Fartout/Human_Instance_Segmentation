{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "185K7kmFBXhTJh5nzT-JzZOb6iev5m2lF",
      "authorship_tag": "ABX9TyMptJPMv7nR2nxTp8oe1b+i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-Fartout/People-Clothing-Segmentation/blob/main/EDA_PCS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data"
      ],
      "metadata": {
        "id": "lMWyTnE7lddM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nxVGbFBjzFn",
        "outputId": "75291321-2bae-4361-f61b-81b6ca473780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading people-clothing-segmentation.zip to /content\n",
            " 99% 612M/616M [00:04<00:00, 124MB/s]\n",
            "100% 616M/616M [00:04<00:00, 138MB/s]\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d rajkumarl/people-clothing-segmentation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/people-clothing-segmentation.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n"
      ],
      "metadata": {
        "id": "6HBFbGD_keZz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!mkdir data/train data/test\n",
        "!mkdir data/train/image data/train/mask data/test/image data/test/mask"
      ],
      "metadata": {
        "id": "VMKO56PEotrp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd png_images/IMAGES\n",
        "!find . -type f -name \"img_0[0-7][0-9][0-9]\\.png\" -exec cp {} /content/data/train/image \\;\n",
        "!find . -type f -name \"img_0[8-9][0-9][0-9]\\.png\"  -exec cp {} /content/data/test/image \\;\n",
        "!find . -type f -name \"img_1000.png\"  -exec cp {} /content/data/test/image \\;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6ROro_zuCwM",
        "outputId": "3bb607aa-e8d4-4ed9-a77c-5ec9b11cc25a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/png_images/IMAGES\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/png_masks/MASKS\n",
        "!find . -type f -name \"seg_0[0-7][0-9][0-9]\\.png\" -exec cp {} /content/data/train/mask \\;\n",
        "!find . -type f -name \"seg_0[8-9][0-9][0-9]\\.png\"  -exec cp {} /content/data/test/mask \\;\n",
        "!find . -type f -name \"seg_1000.png\"  -exec cp {} /content/data/test/mask \\;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRc_S-wtxYvh",
        "outputId": "782d5016-4528-4e73-bd55-74ed9a6db5d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/png_masks/MASKS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning data"
      ],
      "metadata": {
        "id": "BCKVGqg6lhiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_dir = os.path.join(self.root_dir, 'image')\n",
        "        self.mask_dir = os.path.join(self.root_dir, 'mask')\n",
        "        self.image_filenames = os.listdir(self.image_dir)\n",
        "        self.mask_filenames = os.listdir(self.mask_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load image and mask\n",
        "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
        "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "        # apply transformations, if any\n",
        "        if self.transform is not None:\n",
        "            for t in self.transform:\n",
        "                image, mask = self.transform(image, mask)\n",
        "        \n",
        "        # convert PIL Image to tensor\n",
        "        image = torch.tensor(np.array(image).transpose(2, 0, 1)).float()\n",
        "        mask = torch.tensor(np.array(mask)).long()\n",
        "\n",
        "        return image, mask\n"
      ],
      "metadata": {
        "id": "EATC1H2cwHe7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "def brightness( x, y):\n",
        "        x = TF.adjust_brightness(x, 0.1)\n",
        "        return x, y\n",
        "    \n",
        "def gamma(x, y):\n",
        "        x = TF.adjust_gamma(x, 0.1)\n",
        "        return x, y\n",
        "    \n",
        "def hue( x, y):\n",
        "        x = TF.adjust_hue(x, -0.1)\n",
        "        return x, y\n",
        "    \n",
        "def crop(x, y):\n",
        "        # convert to PIL images\n",
        "        x = TF.to_pil_image(x)\n",
        "        y = TF.to_pil_image(y)\n",
        "        # perform central crop\n",
        "        i, j, h, w = TF.resized_crop(x, 0, 0, min(x.size), min(x.size), size=(128, 128))\n",
        "        x = TF.resized_crop(x, i, j, h, w, size=(128, 128))\n",
        "        y = TF.resized_crop(y, i, j, h, w, size=(128, 128))\n",
        "        # convert back to tensors\n",
        "        x = TF.to_tensor(x)\n",
        "        y = TF.to_tensor(y)\n",
        "        # cast mask to integers\n",
        "        y = y.type(torch.uint8)\n",
        "        return x, y\n",
        "    \n",
        "def flip_hori(x, y):\n",
        "        x = TF.hflip(x)\n",
        "        y = TF.hflip(y)\n",
        "        return x, y\n",
        "    \n",
        "def flip_vert(x, y):\n",
        "        x = TF.vflip(x)\n",
        "        y = TF.vflip(y)\n",
        "        return x, y\n",
        "    \n",
        "def rotate(x, y):\n",
        "        # convert to PIL images\n",
        "        x = TF.to_pil_image(x)\n",
        "        y = TF.to_pil_image(y)\n",
        "        # perform rotation\n",
        "        x = TF.rotate(x, 90)\n",
        "        y = TF.rotate(y, 90)\n",
        "        # convert back to tensors\n",
        "        x = TF.to_tensor(x)\n",
        "        y = TF.to_tensor(y)\n",
        "        # cast mask to integers\n",
        "        y = y.type(torch.uint8)\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "lu-eMDStLuKk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmentations = [\n",
        "  brightness,\n",
        "  gamma,\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "oYq18lW4hKsc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import DatasetFolder\n",
        "from torchvision.transforms import transforms\n",
        "import torchvision\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = DatasetFolder('/content/data/train', loader=torchvision.io.image.read_image, extensions=('png',), transform=augmentations)\n",
        "val_dataset = DatasetFolder('/content/data/test', loader=torchvision.io.image.read_image, extensions=('png',))"
      ],
      "metadata": {
        "id": "AoZl-MyLNjUM"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXF-IAyLZcxP",
        "outputId": "0c34fdf8-2e07-4571-c15e-75b611f8d2df"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "    \n",
        "    \n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DownBlock, self).__init__()\n",
        "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
        "        self.down_sample = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_out = self.double_conv(x)\n",
        "        down_out = self.down_sample(skip_out)\n",
        "        return (down_out, skip_out)\n",
        "\n",
        "    \n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, up_sample_mode):\n",
        "        super(UpBlock, self).__init__()\n",
        "        if up_sample_mode == 'conv_transpose':\n",
        "            self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n",
        "        elif up_sample_mode == 'bilinear':\n",
        "            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n",
        "        self.double_conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, down_input, skip_input):\n",
        "        x = self.up_sample(down_input)\n",
        "        x = torch.cat([x, skip_input], dim=1)\n",
        "        return self.double_conv(x)\n",
        "\n",
        "    \n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, out_classes=2, up_sample_mode='conv_transpose'):\n",
        "        super(UNet, self).__init__()\n",
        "        self.up_sample_mode = up_sample_mode\n",
        "        # Downsampling Path\n",
        "        self.down_conv1 = DownBlock(3, 64)\n",
        "        self.down_conv2 = DownBlock(64, 128)\n",
        "        self.down_conv3 = DownBlock(128, 256)\n",
        "        self.down_conv4 = DownBlock(256, 512)\n",
        "        # Bottleneck\n",
        "        self.double_conv = DoubleConv(512, 1024)\n",
        "        # Upsampling Path\n",
        "        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode)\n",
        "        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode)\n",
        "        self.up_conv2 = UpBlock(128 + 256, 128, self.up_sample_mode)\n",
        "        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n",
        "        # Final Convolution\n",
        "        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, skip1_out = self.down_conv1(x)\n",
        "        x, skip2_out = self.down_conv2(x)\n",
        "        x, skip3_out = self.down_conv3(x)\n",
        "        x, skip4_out = self.down_conv4(x)\n",
        "        x = self.double_conv(x)\n",
        "        x = self.up_conv4(x, skip4_out)\n",
        "        x = self.up_conv3(x, skip3_out)\n",
        "        x = self.up_conv2(x, skip2_out)\n",
        "        x = self.up_conv1(x, skip1_out)\n",
        "        x = self.conv_last(x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e8GyFEkdnbQi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get UNet model\n",
        "model = UNet()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "bLhzGhE2DUwj"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}